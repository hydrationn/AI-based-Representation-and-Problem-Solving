{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hydrationn/AI-based-Representation-and-Problem-Solving/blob/main/20240320_3%EC%A3%BC%EC%B0%A8_%EC%8B%A4%EC%8A%B5_1_sequence_to_sequence_for_addition.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "735a2c07-6839-4e09-bcbf-dadb04c3aa7a",
      "metadata": {
        "id": "735a2c07-6839-4e09-bcbf-dadb04c3aa7a"
      },
      "source": [
        "# Math-Addition using Sequence-to-Sequence Learning\n",
        "\n",
        "- Author : Sangkeun Jung (hugmanskj@gmail.com)\n",
        "\n",
        "> Educational Purpose\n",
        "\n",
        "\n",
        "This educational exercise is designed as practical implementation code to facilitate the understanding of **sequence-to-sequence learning**. It represents the first project in a series of two. In this project, we will address a straightforward math-addition problem using sequence-to-sequence learning techniques.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task Description\n",
        "\n",
        "The primary objective of this project is to demonstrate the application of sequence-to-sequence (Seq2Seq) models in solving simple arithmetic problems, specifically addition. Seq2Seq models are a type of neural network architecture designed to handle sequence-to-sequence transformations, making them suitable for tasks like machine translation, text summarization, and, as demonstrated here, arithmetic problem solving.\n",
        "\n",
        "<img src=\"https://www.dropbox.com/scl/fi/b3nknwm1fd9o0ynqj9cw4/addition_seq2seq.png?rlkey=vbfugqnq6n57tq0vrkky16c6i&dl=1\" alt=\"archtecture\" width=\"400\" height=\"auto\">\n",
        "\n",
        "\n",
        "\n",
        "### Overview\n",
        "\n",
        "The task involves creating a model capable of taking a string representation of a simple addition problem, such as \"123+456\", and outputting the correct sum, \"579\". This requires the model to understand both the concept of addition and how to parse and generate numerical sequences. The challenge lies not only in accurately performing the arithmetic but also in managing variable-length input and output sequences, a common hurdle in sequence-to-sequence learning.\n",
        "\n",
        "### Goals\n",
        "\n",
        "1. **Data Generation and Preprocessing**: Develop a method to generate a dataset of addition problems, ensuring a wide range of sums and operand lengths. This step includes preprocessing the data into a format suitable for Seq2Seq learning, such as converting characters to integers and padding sequences for consistent lengths.\n",
        "\n",
        "2. **Model Architecture Design**: Design a Seq2Seq model with an encoder-decoder architecture. The encoder processes the input sequence (the addition problem), and the decoder generates the output sequence (the sum). This involves decisions about the type of layers (e.g., LSTM, GRU), embedding dimensions, hidden state sizes, and layer counts.\n",
        "\n",
        "\n",
        "3. **Training and Optimization**: Implement a training loop to optimize the model parameters using a suitable loss function (e.g., Cross-Entropy Loss) and optimization algorithm (e.g., AdamW). This includes managing aspects like batching and sequence padding.\n",
        "\n",
        "4. **Evaluation and Testing**: Evaluate the model's performance on unseen data, testing its ability to handle a range of addition problems, from simple to more complex. This step assesses the model's generalization capability and its robustness to different input lengths and numerical ranges.\n",
        "\n",
        "### Significance\n",
        "\n",
        "This task serves as an educational tool for understanding Seq2Seq models and their applications beyond traditional areas like language processing. It highlights the versatility of neural networks in learning to perform tasks that require both understanding the structure of the input data and applying logical operations to generate correct outputs.\n",
        "\n",
        "By accomplishing this task, we gain insights into the challenges and solutions in sequence-to-sequence learning, paving the way for more complex applications, such as solving other types of mathematical problems, processing sequences in scientific computing, or even creating models for code generation.\n"
      ],
      "metadata": {
        "id": "UX5gjIzMX6Z8"
      },
      "id": "UX5gjIzMX6Z8"
    },
    {
      "cell_type": "markdown",
      "id": "a4594425-6168-48b5-bfc7-85f8acb23834",
      "metadata": {
        "tags": [],
        "id": "a4594425-6168-48b5-bfc7-85f8acb23834"
      },
      "source": [
        "## Data Creation\n",
        "\n",
        "This section is dedicated to generating the dataset that will be used to train the neural network. The dataset consists of simple addition problems, each involving the sum of two integers ranging from one to three digits. The key steps involved in this process are as follows:\n",
        "\n",
        "1. **Library Imports**: Essential libraries (`numpy`, `random`, `pandas`) are imported for data manipulation and random number generation.\n",
        "2. **Seed Fixation**: The seed for random number generation is fixed to ensure reproducibility of results. This is crucial for maintaining consistency in experiments and obtaining the same outcomes across different runs.\n",
        "3. **`generate_dataset` Function**: This function creates addition problems where the sum of two numbers is less than 9999. Each problem is composed of a string-form input (e.g., \"123+456\") and its corresponding result (e.g., 579). The function proceeds through the following steps:\n",
        "   - A loop is utilized to generate a specified number (`N`) of addition problems.\n",
        "   - In each iteration, two random numbers are generated using `random.randint`.   \n",
        "   - Each generated problem is converted into a tuple of `(input, output)` and added to the final dataset.\n",
        "4. **DataFrame Creation**: A `pandas` DataFrame is created to house all the addition problems. This DataFrame will later be transformed into a data loader for model training purposes.\n",
        "\n",
        "Through this process, a structured dataset is prepared for the model to learn from. Generating the dataset is the first step in model training, enabling the model to learn how to solve addition problems."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# numpy: 숫자를 다룸\n",
        "# pandas: data frame. 엑셀 같은 느낌\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "import random\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "v3vubWcv7A5U"
      },
      "id": "v3vubWcv7A5U",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "cb87016a-0029-4d75-adc8-7bcad9aabc4e",
      "metadata": {
        "tags": [],
        "id": "cb87016a-0029-4d75-adc8-7bcad9aabc4e"
      },
      "outputs": [],
      "source": [
        "# seed 고정시키고, 다른 사람이 돌렸을 때에도 동일하게 작동되도록 함. 머신러닝이나 딥러닝에 이용하면 좋음.\n",
        "\n",
        "def set_seed_everything(seed=42):\n",
        "    # Fix the random number generator seed\n",
        "    random.seed(seed)  # Set the seed for Python's random module\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "\n",
        "    # 알아두면 좋다,,\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "    # Use deterministic algorithms and disable benchmark mode\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "set_seed_everything()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "9c2bd993-abc7-40d1-8a52-797d529c3627",
      "metadata": {
        "tags": [],
        "id": "9c2bd993-abc7-40d1-8a52-797d529c3627"
      },
      "outputs": [],
      "source": [
        "# Function to generate dataset\n",
        "def generate_dataset(N=50000):\n",
        "    data = []\n",
        "\n",
        "    for _ in range(N):\n",
        "        # Generate two numbers between 1 and 999 (inclusive)\n",
        "        num1 = random.randint(1, 999)\n",
        "        num2 = random.randint(1, 999)\n",
        "\n",
        "        # Make sure the sum is also less than 1000\n",
        "        while num1 + num2 >= 1000:\n",
        "            num1 = random.randint(1, 999)\n",
        "            num2 = random.randint(1, 999)\n",
        "\n",
        "        # Format the input and output\n",
        "        input_str = f\"{num1}+{num2}\"\n",
        "        output = num1 + num2\n",
        "\n",
        "        # Append to the dataset\n",
        "        data.append((input_str, output))\n",
        "\n",
        "    # Create a DataFrame\n",
        "    df = pd.DataFrame(data, columns=['Input', 'Output'])\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate the dataset\n",
        "df = generate_dataset()\n",
        "\n",
        "# Display the first few rows of the DataFrame\n",
        "print( df.head() )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ateru02E68vt",
        "outputId": "f819af7c-5567-4942-d529-e15a9fb57845"
      },
      "id": "Ateru02E68vt",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     Input  Output\n",
            "0  655+115     770\n",
            "1   26+760     786\n",
            "2  282+251     533\n",
            "3  229+143     372\n",
            "4  755+105     860\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Preparation and DataLoader\n",
        "\n",
        "Here, we define the `AdditionDataset` class, which prepares our data for the sequence-to-sequence model. It involves converting characters to integers and padding the sequences. We then create a DataLoader to batch and shuffle our data, making it ready for training.\n"
      ],
      "metadata": {
        "id": "Ob4Z3krhYusC"
      },
      "id": "Ob4Z3krhYusC"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "ef0d27cb-8a5e-4f38-9391-6b7201ea6bcd",
      "metadata": {
        "tags": [],
        "id": "ef0d27cb-8a5e-4f38-9391-6b7201ea6bcd"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class AdditionDataset(Dataset):\n",
        "    def __init__(self, df):\n",
        "        self.df = df\n",
        "\n",
        "        # Dictionaries mapping characters to numbers, adding '#' as a padding symbol\n",
        "        self.input_char_to_int = {\n",
        "            '0': 0, '1': 1, '2': 2, '3': 3, '4': 4,\n",
        "            '5': 5, '6': 6, '7': 7, '8': 8, '9': 9,\n",
        "            '+': 10, '#': 11\n",
        "        }\n",
        "        self.output_char_to_int = {\n",
        "            '0': 0, '1': 1, '2': 2, '3': 3, '4': 4,\n",
        "            '5': 5, '6': 6, '7': 7, '8': 8, '9': 9,\n",
        "            '#': 10\n",
        "        }\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.tolist()\n",
        "\n",
        "        # Extract input and output data\n",
        "        input_str, output = self.df.iloc[idx]\n",
        "        input_data  = [self.input_char_to_int[char] for char in input_str]\n",
        "        output_data = [self.output_char_to_int[char] for char in str(output)]\n",
        "\n",
        "        # Pad input data to 7 characters, and output data to 4 characters\n",
        "        input_data  += [self.input_char_to_int['#']] * (7 - len(input_data))   # Input is 7 characters\n",
        "        output_data += [self.output_char_to_int['#']] * (4 - len(output_data)) # Output is up to 4 characters\n",
        "\n",
        "        return torch.tensor(input_data, dtype=torch.long), torch.tensor(output_data, dtype=torch.long)\n",
        "\n",
        "\n",
        "# Create an instance of the Dataset\n",
        "addition_dataset = AdditionDataset(df)\n",
        "\n",
        "# Set up DataLoader\n",
        "batch_size = 2048\n",
        "addition_dataloader = DataLoader(addition_dataset, batch_size=batch_size, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Architecture\n",
        "\n",
        "The model consists of three main components: an encoder, a decoder, and the Seq2Seq model that integrates both. The encoder embeds and processes the input sequence, the decoder generates the output sequence, and the Seq2Seq model orchestrates the flow from input to output.\n",
        "\n"
      ],
      "metadata": {
        "id": "zAF0mObWfVpr"
      },
      "id": "zAF0mObWfVpr"
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://www.dropbox.com/scl/fi/s1f14impbm64o5qxgzoxp/addition_rnn.png?rlkey=1q3f3irg4a2gudn7hb09y8o8j&dl=1\" alt=\"archtecture\" width=\"400\" height=\"auto\">\n"
      ],
      "metadata": {
        "id": "hcJ6dh2i3ML7"
      },
      "id": "hcJ6dh2i3ML7"
    },
    {
      "cell_type": "code",
      "source": [
        "# Encoder Definition\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim, emb_dim, hidden_dim, n_layers):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(input_dim, emb_dim)  # Embedding layer\n",
        "        self.rnn = nn.LSTM(emb_dim, hidden_dim, n_layers, batch_first=True)  # RNN layer\n",
        "\n",
        "    def forward(self, src):\n",
        "        embedded = self.embedding(src)  # Convert input tokens to embeddings\n",
        "        output, (hidden, cell) = self.rnn(embedded)  # Forward pass through RNN\n",
        "\n",
        "        # In LSTM, h_n and c_n store the hidden state and cell state of the last time step, respectively.\n",
        "        # For more information, you can check the PyTorch documentation for LSTM\n",
        "        # at: https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html\n",
        "        return hidden"
      ],
      "metadata": {
        "id": "9StlObsD70KZ"
      },
      "id": "9StlObsD70KZ",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Decoder Definition\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, output_dim, emb_dim, hidden_dim, n_layers):\n",
        "        super().__init__()\n",
        "        self.output_dim = output_dim\n",
        "        self.rnn = nn.LSTM(emb_dim, hidden_dim, n_layers, batch_first=True)  # RNN layer\n",
        "        self.fc_out = nn.Linear(hidden_dim, output_dim)  # Output layer\n",
        "\n",
        "\n",
        "    def forward(self, input):\n",
        "        output, (hidden, cell) = self.rnn(input)  # Forward pass through RNN\n",
        "        prediction = self.fc_out(output)  # Predict next token\n",
        "        return prediction"
      ],
      "metadata": {
        "id": "TfpB7JlG7-Yq"
      },
      "id": "TfpB7JlG7-Yq",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Seq2Seq Model Integration\n",
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder, device):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.device = device\n",
        "\n",
        "    def forward(self, src, trg):\n",
        "        trg_len = trg.shape[1]\n",
        "        hidden = self.encoder(src)  # Initial hidden state from encoder.\n",
        "\n",
        "        # get top layer hidden states\n",
        "        last_layer_hidden = hidden[-1] # [batch_size, dim]\n",
        "\n",
        "        # copy\n",
        "        dec_input = last_layer_hidden.unsqueeze(1).expand(-1, trg_len, -1)\n",
        "        # dec_input : [batch_size, target_length, dim]\n",
        "        output = self.decoder(dec_input)\n",
        "\n",
        "        return output  # [batch_size, target_length, num_output_label]\n"
      ],
      "metadata": {
        "id": "EsHLyTuT7-Qx"
      },
      "id": "EsHLyTuT7-Qx",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Initialization and Loss Function\n",
        "\n",
        "We initialize the encoder, decoder, and Seq2Seq model with specified dimensions. An AdamW optimizer and CrossEntropyLoss function are also defined to optimize our model during training.\n"
      ],
      "metadata": {
        "id": "GAUqk14IgvJ7"
      },
      "id": "GAUqk14IgvJ7"
    },
    {
      "cell_type": "code",
      "source": [
        "### Model Initialization\n",
        "# Initialize the model components with the specified dimensions and layers\n",
        "INPUT_DIM = 12  # Number of unique tokens in the input\n",
        "OUTPUT_DIM = 11  # Number of unique tokens in the output (+1 for padding token)\n",
        "EMB_DIM = 200\n",
        "HID_DIM = 200\n",
        "ENC_LAYERS = 1\n",
        "DEC_LAYERS = 3\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(DEVICE)\n",
        "\n",
        "encoder = Encoder(INPUT_DIM, EMB_DIM, HID_DIM, ENC_LAYERS).to(DEVICE)\n",
        "decoder = Decoder(OUTPUT_DIM, EMB_DIM, HID_DIM, DEC_LAYERS).to(DEVICE)\n",
        "model = Seq2Seq(encoder, decoder, DEVICE).to(DEVICE)\n",
        "\n",
        "# Define the optimizer and loss function\n",
        "optimizer = optim.AdamW(model.parameters(), lr=0.001)\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "zqYBrIU3c1xr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14dd7746-8158-4b7c-ebe2-331a0f8c4490"
      },
      "id": "zqYBrIU3c1xr",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Loop\n",
        "\n",
        "This section outlines the training process, during which the model is trained for a specified number of epochs. Loss is calculated at each step to monitor the training progress and ensure the model is learning effectively.\n"
      ],
      "metadata": {
        "id": "Is4jcplgg2pv"
      },
      "id": "Is4jcplgg2pv"
    },
    {
      "cell_type": "code",
      "source": [
        "### Training Loop\n",
        "def train_model(model, dataloader, optimizer, criterion, epochs):\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        epoch_loss = 0\n",
        "\n",
        "        for src, trg in dataloader:\n",
        "            src, trg = src.to(DEVICE), trg.to(DEVICE)\n",
        "            optimizer.zero_grad()  #  <-- FOR PYTORCH\n",
        "\n",
        "            # ---------------------------------------------\n",
        "            output = model(src, trg)\n",
        "\n",
        "            # reshape output for loss calculation\n",
        "            output = output.transpose(1,2)\n",
        "            loss = criterion(output, trg)\n",
        "            # ---------------------------------------------\n",
        "\n",
        "            loss.backward()  #  <-- FOR PYTORCH\n",
        "            optimizer.step() #  <-- FOR PYTORCH\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "        print(f'Epoch: {epoch+1:02}, Loss: {epoch_loss / len(dataloader):.4f}')"
      ],
      "metadata": {
        "id": "k_Rb3dXGesgZ"
      },
      "id": "k_Rb3dXGesgZ",
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uq04QU2EA5Y0",
        "outputId": "50cfc434-3c8b-4d8a-c9c5-2af7ad9a21d3"
      },
      "id": "Uq04QU2EA5Y0",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training Loop\n",
        "EPOCHS = 100\n",
        "train_model(model, addition_dataloader, optimizer, criterion, EPOCHS)\n",
        "torch.save(model.state_dict(), 'addition_model.pth')"
      ],
      "metadata": {
        "id": "okGTiour8aWt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31d13d34-160a-4591-ab3f-710e1165a818"
      },
      "id": "okGTiour8aWt",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 01, Loss: 2.1086\n",
            "Epoch: 02, Loss: 1.8544\n",
            "Epoch: 03, Loss: 1.7332\n",
            "Epoch: 04, Loss: 1.6864\n",
            "Epoch: 05, Loss: 1.6728\n",
            "Epoch: 06, Loss: 1.6590\n",
            "Epoch: 07, Loss: 1.6129\n",
            "Epoch: 08, Loss: 1.5213\n",
            "Epoch: 09, Loss: 1.4146\n",
            "Epoch: 10, Loss: 1.3473\n",
            "Epoch: 11, Loss: 1.2994\n",
            "Epoch: 12, Loss: 1.2550\n",
            "Epoch: 13, Loss: 1.2388\n",
            "Epoch: 14, Loss: 1.2120\n",
            "Epoch: 15, Loss: 1.1897\n",
            "Epoch: 16, Loss: 1.1912\n",
            "Epoch: 17, Loss: 1.1468\n",
            "Epoch: 18, Loss: 1.1256\n",
            "Epoch: 19, Loss: 1.0883\n",
            "Epoch: 20, Loss: 1.0684\n",
            "Epoch: 21, Loss: 1.0317\n",
            "Epoch: 22, Loss: 1.0216\n",
            "Epoch: 23, Loss: 0.9735\n",
            "Epoch: 24, Loss: 0.9586\n",
            "Epoch: 25, Loss: 0.9192\n",
            "Epoch: 26, Loss: 0.8868\n",
            "Epoch: 27, Loss: 0.8946\n",
            "Epoch: 28, Loss: 0.8674\n",
            "Epoch: 29, Loss: 0.8457\n",
            "Epoch: 30, Loss: 0.8622\n",
            "Epoch: 31, Loss: 0.8289\n",
            "Epoch: 32, Loss: 0.8241\n",
            "Epoch: 33, Loss: 0.8273\n",
            "Epoch: 34, Loss: 0.8009\n",
            "Epoch: 35, Loss: 0.7952\n",
            "Epoch: 36, Loss: 0.7877\n",
            "Epoch: 37, Loss: 0.7980\n",
            "Epoch: 38, Loss: 0.7787\n",
            "Epoch: 39, Loss: 0.7709\n",
            "Epoch: 40, Loss: 0.7549\n",
            "Epoch: 41, Loss: 0.7786\n",
            "Epoch: 42, Loss: 0.7406\n",
            "Epoch: 43, Loss: 0.7634\n",
            "Epoch: 44, Loss: 0.7341\n",
            "Epoch: 45, Loss: 0.7407\n",
            "Epoch: 46, Loss: 0.7585\n",
            "Epoch: 47, Loss: 0.7094\n",
            "Epoch: 48, Loss: 0.6638\n",
            "Epoch: 49, Loss: 0.7028\n",
            "Epoch: 50, Loss: 0.6107\n",
            "Epoch: 51, Loss: 0.6131\n",
            "Epoch: 52, Loss: 0.5474\n",
            "Epoch: 53, Loss: 0.5051\n",
            "Epoch: 54, Loss: 0.4985\n",
            "Epoch: 55, Loss: 0.4594\n",
            "Epoch: 56, Loss: 0.4041\n",
            "Epoch: 57, Loss: 0.5209\n",
            "Epoch: 58, Loss: 0.4062\n",
            "Epoch: 59, Loss: 0.3506\n",
            "Epoch: 60, Loss: 0.4320\n",
            "Epoch: 61, Loss: 0.3423\n",
            "Epoch: 62, Loss: 0.3112\n",
            "Epoch: 63, Loss: 0.3468\n",
            "Epoch: 64, Loss: 0.3905\n",
            "Epoch: 65, Loss: 0.2740\n",
            "Epoch: 66, Loss: 0.2771\n",
            "Epoch: 67, Loss: 0.2555\n",
            "Epoch: 68, Loss: 0.3610\n",
            "Epoch: 69, Loss: 0.2785\n",
            "Epoch: 70, Loss: 0.2508\n",
            "Epoch: 71, Loss: 0.2074\n",
            "Epoch: 72, Loss: 0.2296\n",
            "Epoch: 73, Loss: 0.2133\n",
            "Epoch: 74, Loss: 0.1819\n",
            "Epoch: 75, Loss: 0.1741\n",
            "Epoch: 76, Loss: 0.3946\n",
            "Epoch: 77, Loss: 0.3143\n",
            "Epoch: 78, Loss: 0.1719\n",
            "Epoch: 79, Loss: 0.1496\n",
            "Epoch: 80, Loss: 0.1408\n",
            "Epoch: 81, Loss: 0.1319\n",
            "Epoch: 82, Loss: 0.1326\n",
            "Epoch: 83, Loss: 0.1371\n",
            "Epoch: 84, Loss: 0.3998\n",
            "Epoch: 85, Loss: 0.2802\n",
            "Epoch: 86, Loss: 0.1419\n",
            "Epoch: 87, Loss: 0.1137\n",
            "Epoch: 88, Loss: 0.1055\n",
            "Epoch: 89, Loss: 0.1137\n",
            "Epoch: 90, Loss: 0.1046\n",
            "Epoch: 91, Loss: 0.0996\n",
            "Epoch: 92, Loss: 0.0892\n",
            "Epoch: 93, Loss: 0.0844\n",
            "Epoch: 94, Loss: 0.5454\n",
            "Epoch: 95, Loss: 0.2747\n",
            "Epoch: 96, Loss: 0.1268\n",
            "Epoch: 97, Loss: 0.0922\n",
            "Epoch: 98, Loss: 0.0817\n",
            "Epoch: 99, Loss: 0.0728\n",
            "Epoch: 100, Loss: 0.0679\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing and Inference\n",
        "\n",
        "After training, the model is tested on new addition problems to evaluate its performance. The `test_model` function handles the inference process, converting input strings to tensors, making predictions with the model, and converting the numeric output back to strings.\n",
        "\n"
      ],
      "metadata": {
        "id": "vKiRMxVahHeF"
      },
      "id": "vKiRMxVahHeF"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading the Model"
      ],
      "metadata": {
        "id": "AxOJJnQnhUw5"
      },
      "id": "AxOJJnQnhUw5"
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the saved model\n",
        "model.load_state_dict(torch.load('addition_model.pth'))"
      ],
      "metadata": {
        "id": "on1mwCTmhH3c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0616d054-1f6e-4bf2-a7b1-8ff416dab7c8"
      },
      "id": "on1mwCTmhH3c",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Inference(Addition)"
      ],
      "metadata": {
        "id": "I6_HV2iQkol6"
      },
      "id": "I6_HV2iQkol6"
    },
    {
      "cell_type": "code",
      "source": [
        "input_char_to_int  = addition_dataset.input_char_to_int\n",
        "output_int_to_char = {idx:char for char, idx in addition_dataset.output_char_to_int.items() }"
      ],
      "metadata": {
        "id": "4mL2ARr3-Tgo"
      },
      "id": "4mL2ARr3-Tgo",
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_char_to_int"
      ],
      "metadata": {
        "id": "jk6Z3Q-J-V43",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ec13656-7e02-4809-86a3-a7d5faa188ed"
      },
      "id": "jk6Z3Q-J-V43",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'0': 0,\n",
              " '1': 1,\n",
              " '2': 2,\n",
              " '3': 3,\n",
              " '4': 4,\n",
              " '5': 5,\n",
              " '6': 6,\n",
              " '7': 7,\n",
              " '8': 8,\n",
              " '9': 9,\n",
              " '+': 10,\n",
              " '#': 11}"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output_int_to_char"
      ],
      "metadata": {
        "id": "Ll1wubIb-YJA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff362c05-6232-4237-de25-b5583a5670fd"
      },
      "id": "Ll1wubIb-YJA",
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: '0',\n",
              " 1: '1',\n",
              " 2: '2',\n",
              " 3: '3',\n",
              " 4: '4',\n",
              " 5: '5',\n",
              " 6: '6',\n",
              " 7: '7',\n",
              " 8: '8',\n",
              " 9: '9',\n",
              " 10: '#'}"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def test_model(model, input_str, device):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        # Convert input string to numbers\n",
        "        input_data = [input_char_to_int[char] for char in input_str]\n",
        "        input_tensor = torch.tensor(input_data, dtype=torch.long).unsqueeze(0).to(device)  # Add batch dimension\n",
        "\n",
        "        # Model prediction\n",
        "        dummy_target = torch.zeros((1, 4), dtype=torch.long).to(device) # Dummy target tensor\n",
        "        output = model(input_tensor, dummy_target)\n",
        "        output = output.argmax(dim=2)  # Select the index with the highest probability\n",
        "        print(output.shape)\n",
        "\n",
        "        # Convert numeric output back to string\n",
        "        output_str = ''.join(output_int_to_char[int(idx)] for idx in output[0])\n",
        "        return output_str\n",
        "        #return output_str.rstrip('#')  # Remove padding"
      ],
      "metadata": {
        "id": "A-CTbX15hSnk"
      },
      "id": "A-CTbX15hSnk",
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def do_add(input_str):\n",
        "  # Prepare input string\n",
        "  input_str = list(input_str)\n",
        "  input_str += ['#'] * (7 - len(input_str))  # padding processing\n",
        "\n",
        "\n",
        "  # Test the model\n",
        "  output_str = test_model(model, input_str, DEVICE)\n",
        "  return output_str"
      ],
      "metadata": {
        "id": "Ilm_q7kg-Kdx"
      },
      "id": "Ilm_q7kg-Kdx",
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_str = \"345+21\"\n",
        "output_str = do_add(input_str)\n",
        "print(f'Input: {input_str}, Output: {output_str}')"
      ],
      "metadata": {
        "id": "Qff9cFTE-NXw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "70ca5279-20ea-476b-df54-3c18cc912472"
      },
      "id": "Qff9cFTE-NXw",
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 4])\n",
            "Input: 345+21, Output: 366#\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_str = \"223+26\"\n",
        "output_str = do_add(input_str)\n",
        "print(f'Input: {input_str}, Output: {output_str}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oGxeU5Z7ORXE",
        "outputId": "6af9f631-824b-4d1c-d399-13536e2ad43c"
      },
      "id": "oGxeU5Z7ORXE",
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 4])\n",
            "Input: 223+26, Output: 249#\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1rzrDsZi3mbl"
      },
      "id": "1rzrDsZi3mbl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "20e05bed-c87c-4f87-981d-b430acffffa4",
      "metadata": {
        "id": "20e05bed-c87c-4f87-981d-b430acffffa4"
      },
      "source": [
        "## Conclusion\n",
        "\n",
        "This notebook demonstrates a practical application of sequence-to-sequence learning for solving simple addition problems. The focus is on the educational aspect, illustrating the process from data preparation to model training and inference. The implementation showcases the ease of using libraries like PyTorch and Hugging Face for building and training machine learning models.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WGxJxWeN-s6t"
      },
      "id": "WGxJxWeN-s6t",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}